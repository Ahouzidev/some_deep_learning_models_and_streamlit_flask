{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "for model_info in genai.list_tuned_models():\n",
    "    print(model_info.name)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T14:48:55.213754Z",
     "start_time": "2025-12-31T14:48:54.737910Z"
    }
   },
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lab: Fine Tuning            \n",
    "Prof. OUHMIDA\n",
    "## T.A.F: \n",
    "<ol>\n",
    "<li>Démarrer le programme </li>\n",
    "<li>Analyser le code </li>\n",
    "<li>Améliorer le programme </li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "source: https://ai.google.dev/gemini-api/docs/model-tuning/tutorial?lang=python&hl=fr\n",
    "\n",
    "pip install -U google-generativeai\n",
    "pip install python-dotenv"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1- Installer le package du SDK et configurer votre clé API"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Importez le package et configurez le service avec votre clé API\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "load_dotenv()  # Load environment variables from .env\n",
    "genai.configure(api_key=os.environ['API_KEY'])"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T14:49:04.234142Z",
     "start_time": "2025-12-31T14:49:04.227882Z"
    }
   },
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "source": [
    "model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "response = model.generate_content(\"Expliquer comment fonctionne Fine Tuning\")\n",
    "print(response.text)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T14:49:22.902192Z",
     "start_time": "2025-12-31T14:49:06.351713Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le **Fine-Tuning** (ou \"réglage fin\" en français) est une technique fondamentale en apprentissage automatique, particulièrement populaire avec les grands modèles (Large Language Models - LLMs, modèles de vision, etc.). Il s'inscrit dans le cadre plus large du **transfer learning (apprentissage par transfert)**.\n",
      "\n",
      "Pour comprendre son fonctionnement, imaginez un élève qui a déjà suivi de nombreuses années d'école générale et possède une vaste connaissance du monde. Le Fine-Tuning, c'est comme spécialiser cet élève dans un domaine très précis (par exemple, devenir un expert en droit des brevets ou en diagnostic médical).\n",
      "\n",
      "Voici une explication détaillée de son fonctionnement :\n",
      "\n",
      "---\n",
      "\n",
      "### La Logique de Base : Transfer Learning\n",
      "\n",
      "Avant le Fine-Tuning, il y a la phase de **pré-entraînement**.\n",
      "1.  **Pré-entraînement (Pre-training) :** Un modèle est d'abord entraîné sur une **énorme quantité de données très diverses et générales** (par exemple, des téraoctets de texte provenant d'internet pour un LLM, ou des millions d'images pour un modèle de vision). L'objectif est qu'il apprenne des représentations générales et de haut niveau, des patterns, des règles et des caractéristiques fondamentales du domaine.\n",
      "    *   *Exemple pour un LLM :* Apprendre la grammaire, la sémantique, le contexte, les faits généraux, la structure des phrases, etc., en prédisant le mot suivant ou en remplissant les blancs.\n",
      "    *   *Exemple pour un modèle de vision :* Apprendre à détecter les bords, les textures, les formes, les objets de base.\n",
      "\n",
      "2.  **Fine-Tuning (Réglage Fin) :** Une fois le modèle pré-entraîné, il possède une base de connaissances solides. Le Fine-Tuning consiste à **prendre ce modèle pré-entraîné et à le continuer à entraîner (ou le ré-entraîner partiellement) sur un ensemble de données beaucoup plus petit et très spécifique à une tâche ou un domaine particulier.**\n",
      "\n",
      "---\n",
      "\n",
      "### Comment ça Fonctionne, Étape par Étape :\n",
      "\n",
      "1.  **Choix du Modèle Pré-entraîné :**\n",
      "    *   Vous commencez par sélectionner un modèle pré-entraîné qui est déjà excellent dans sa tâche générale (par exemple, GPT-3, Llama, BERT, ResNet, Vision Transformer). C'est votre \"élève bien éduqué\".\n",
      "\n",
      "2.  **Préparation du Jeu de Données Spécifique :**\n",
      "    *   Vous assemblez un jeu de données de haute qualité, beaucoup plus petit que celui de pré-entraînement, qui est directement pertinent pour la tâche spécifique que vous voulez que le modèle accomplisse.\n",
      "    *   *Exemple pour un LLM :* Si vous voulez qu'il soit expert en assistance client pour une entreprise X, votre jeu de données pourrait contenir des paires de \"questions fréquentes de clients X\" et \"réponses optimales de l'entreprise X\". Ou des documents internes de l'entreprise.\n",
      "    *   *Exemple pour un modèle de vision :* Si vous voulez détecter des défauts spécifiques sur des pièces industrielles, votre jeu de données contiendra des images de pièces avec et sans défauts étiquetés.\n",
      "\n",
      "3.  **Ajustement des Paramètres d'Entraînement :**\n",
      "    *   **Taux d'apprentissage (Learning Rate) :** C'est un paramètre crucial. Pendant le Fine-Tuning, le taux d'apprentissage est généralement **beaucoup plus petit** que lors du pré-entraînement. Pourquoi ? Parce que vous ne voulez pas \"écraser\" toute la connaissance générale que le modèle a acquise. Vous voulez seulement l'ajuster légèrement pour qu'il s'adapte à la nouvelle tâche, sans oublier ce qu'il sait déjà. Un taux trop élevé pourrait provoquer un \"oubli catastrophique\".\n",
      "    *   **Époques d'entraînement :** Le nombre d'époques (cycles complets sur le jeu de données) est généralement plus faible car le modèle n'a pas besoin d'apprendre de zéro.\n",
      "\n",
      "4.  **Entraînement des Couches du Modèle :**\n",
      "    *   **Option 1 : Entraînement complet (Full Fine-Tuning) :** Toutes les couches du modèle sont mises à jour avec les nouvelles données. C'est l'approche la plus courante et souvent la plus performante, mais aussi la plus gourmande en ressources.\n",
      "    *   **Option 2 : Entraînement partiel (Feature Extraction / Fine-Tuning des dernières couches) :** Pour les modèles de vision plus anciens, il était courant de \"geler\" les premières couches (qui apprennent des caractéristiques génériques comme les bords) et de n'entraîner que les dernières couches (qui sont plus spécifiques à la tâche). Pour les LLMs, cela est moins courant mais des techniques comme le Fine-Tuning des couches d'attention uniquement peuvent exister.\n",
      "    *   **Option 3 : Entraînement Par-couche Efficace (PEFT - Parameter-Efficient Fine-Tuning) :** C'est l'approche la plus moderne et populaire pour les LLMs géants (comme Llama 2, GPT-3.5). Au lieu d'entraîner des milliards de paramètres, des petites \"adaptations\" (des matrices de faible rang) sont ajoutées au modèle pré-entraîné. Seuls les paramètres de ces adaptations sont entraînés, ce qui réduit considérablement les coûts de calcul et de mémoire. Les techniques les plus connues sont **LoRA (Low-Rank Adaptation)**, QLoRA, ou AdaLoRA.\n",
      "\n",
      "5.  **Évaluation :**\n",
      "    *   Une fois le Fine-Tuning terminé, le modèle est évalué sur un ensemble de données de test spécifique à la nouvelle tâche pour mesurer ses performances et s'assurer qu'il a bien appris la spécialisation souhaitée.\n",
      "\n",
      "---\n",
      "\n",
      "### Pourquoi le Fine-Tuning est-il Puissant ?\n",
      "\n",
      "1.  **Capitalisation des Connaissances :** Il permet de tirer parti de la vaste connaissance et des capacités de généralisation que le modèle a acquises pendant le pré-entraînement, plutôt que de partir de zéro.\n",
      "2.  **Moins de Données Spécifiques Requises :** Entraîner un modèle de zéro pour une tâche spécifique nécessiterait un jeu de données énorme et coûteux. Le Fine-Tuning ne demande qu'un jeu de données spécifique beaucoup plus petit, car le modèle n'a qu'à \"ajuster\" ses connaissances existantes.\n",
      "3.  **Réduction des Coûts et du Temps :** Il est exponentiellement moins cher et plus rapide d'effectuer un Fine-Tuning que de pré-entraîner un modèle entier.\n",
      "4.  **Meilleures Performances :** Pour la plupart des tâches spécifiques, un modèle Fine-Tuné surpassera largement un modèle générique non Fine-Tuné, car il est optimisé pour les nuances de cette tâche.\n",
      "5.  **Personnalisation :** Il permet de créer des modèles sur mesure pour des besoins très spécifiques (une marque, un jargon technique, un style d'écriture particulier, une tâche de vision très nichée).\n",
      "\n",
      "---\n",
      "\n",
      "### En Résumé :\n",
      "\n",
      "Le Fine-Tuning est le processus de prendre un modèle d'IA très intelligent et polyvalent (pré-entraîné sur de vastes données générales) et de lui donner une \"formation spécialisée\" sur un petit ensemble de données spécifiques à une tâche. Cela lui permet d'adapter et d'affiner ses connaissances pour exceller dans cette tâche particulière, tout en conservant la compréhension générale acquise précédemment. C'est un pont essentiel entre les capacités générales des grands modèles et les besoins spécifiques des applications du monde réel.\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2- Répertorier les tuned-models(modèles affinés)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "for model_info in genai.list_tuned_models():\n",
    "    print(model_info.name)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T14:49:28.989082Z",
     "start_time": "2025-12-31T14:49:28.481794Z"
    }
   },
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3- Créer un Tuned-Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import google.generativeai as genai\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "base_model = \"models/gemini-1.5-flash-001-tuning\"\n",
    "training_data = [\n",
    "    {\"text_input\": \"1\", \"output\": \"2\"},\n",
    "    {\"text_input\": \"2\", \"output\": \"3\"},\n",
    "    {\"text_input\": \"3\", \"output\": \"4\"},\n",
    "    {\"text_input\": \"10\", \"output\": \"11\"},\n",
    "    {\"text_input\": \"12\", \"output\": \"13\"},\n",
    "    {\"text_input\": \"100\", \"output\": \"101\"},\n",
    "    {\"text_input\": \"1000\", \"output\": \"1001\"},\n",
    "    {\"text_input\": \"I\", \"output\": \"II\"},\n",
    "    {\"text_input\": \"II\", \"output\": \"III\"},\n",
    "    {\"text_input\": \"III\", \"output\": \"IV\"},\n",
    "    {\"text_input\": \"IV\", \"output\": \"V\"},\n",
    "    {\"text_input\": \"five\", \"output\": \"six\"},\n",
    "    {\"text_input\": \"six\", \"output\": \"seven\"},\n",
    "    {\"text_input\": \"seven\", \"output\": \"eight\"},\n",
    "    {\"text_input\": \"eleven\", \"output\": \"twelve\"},\n",
    "    {\"text_input\": \"thirteen\", \"output\": \"fourteen\"},\n",
    "    {\"text_input\": \"nineteen\", \"output\": \"twenty\"}\n",
    "]\n",
    "\n",
    "try:\n",
    "    operation = genai.create_tuned_model(\n",
    "        display_name=\"increment\",\n",
    "        source_model=base_model,\n",
    "        epoch_count=10,\n",
    "        batch_size=4,\n",
    "        learning_rate=0.3,\n",
    "        training_data=training_data,\n",
    "    )\n",
    "\n",
    "\n",
    "    for status in operation.wait_bar():\n",
    "        logging.debug(f\"Status object: {vars(status)}\") # To inspect what info we have available\n",
    "        if hasattr(status, 'done') and status.done:\n",
    "            logging.info(\"Fine-tuning operation completed.\")\n",
    "            break\n",
    "\n",
    "        if hasattr(status, 'error') and status.error:\n",
    "            logging.error(f\"Fine-tuning failed with error: {status.error}\")\n",
    "            break\n",
    "        # You can add some logging here to track progress. It can be different for different operations\n",
    "        if hasattr(status, 'progress'):\n",
    "            logging.info(f\"Progress: {status.progress}\")\n",
    "\n",
    "\n",
    "    result = operation.result()\n",
    "    logging.info(f\"Fine-tuning complete. Tuned model name: {result.name}\")\n",
    "\n",
    "    model = genai.GenerativeModel(model_name=result.name)\n",
    "    result = model.generate_content(\"III\")\n",
    "    logging.info(f\"Generated output: {result.text}\")  # Expected: IV\n",
    "    #Basic test to see if it gives us what we expect.\n",
    "    if \"IV\" in result.text:\n",
    "        logging.info(\"Sanity check passed, model is working.\")\n",
    "    else:\n",
    "        logging.warning(f\"Sanity check failed, model did not return expected result: {result.text}\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred: {e}\")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T15:41:01.270050Z",
     "start_time": "2025-12-31T15:41:01.141393Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:An error occurred: 400 models/gemini-2.5-flash is not found for CREATE TUNED MODEL at API version v1beta.\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "import time\n",
    "\n",
    "base_model = \"models/gemini-1.5-flash-001-tuning\"\n",
    "training_data = [\n",
    "    {\"text_input\": \"1\", \"output\": \"2\"},\n",
    "    {\"text_input\": \"2\", \"output\": \"3\"},\n",
    "    {\"text_input\": \"3\", \"output\": \"4\"},\n",
    "    {\"text_input\": \"10\", \"output\": \"11\"},\n",
    "    {\"text_input\": \"12\", \"output\": \"13\"},\n",
    "    {\"text_input\": \"100\", \"output\": \"101\"},\n",
    "    {\"text_input\": \"1000\", \"output\": \"1001\"},\n",
    "    {\"text_input\": \"I\", \"output\": \"II\"},\n",
    "    {\"text_input\": \"II\", \"output\": \"III\"},\n",
    "    {\"text_input\": \"III\", \"output\": \"IV\"},\n",
    "    {\"text_input\": \"IV\", \"output\": \"V\"},\n",
    "    {\"text_input\": \"five\", \"output\": \"six\"},\n",
    "    {\"text_input\": \"six\", \"output\": \"seven\"},\n",
    "    {\"text_input\": \"seven\", \"output\": \"eight\"},\n",
    "    {\"text_input\": \"eleven\", \"output\": \"twelve\"},\n",
    "    {\"text_input\": \"thirteen\", \"output\": \"fourteen\"},\n",
    "    {\"text_input\": \"nineteen\", \"output\": \"twenty\"}\n",
    "]\n",
    "\n",
    "operation = genai.create_tuned_model(\n",
    "    # You can use a tuned model here too. Set `source_model=\"tunedModels/...\"`\n",
    "    display_name=\"increment\",\n",
    "    source_model=base_model,\n",
    "    epoch_count=10,\n",
    "    batch_size=4,\n",
    "    learning_rate=0.1,\n",
    "    training_data=training_data,\n",
    ")\n",
    "\n",
    "for status in operation.wait_bar():\n",
    "    time.sleep(10)\n",
    "\n",
    "result = operation.result()\n",
    "print(result)\n",
    "# # You can plot the loss curve with:\n",
    "# snapshots = pd.DataFrame(result.tuning_task.snapshots)\n",
    "# sns.lineplot(data=snapshots, x='epoch', y='mean_loss')\n",
    "\n",
    "model = genai.GenerativeModel(model_name=result.name)\n",
    "result = model.generate_content(\"III\")\n",
    "print(result.text)  # IV"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T15:41:11.677804Z",
     "start_time": "2025-12-31T15:41:11.298860Z"
    }
   },
   "outputs": [
    {
     "ename": "FailedPrecondition",
     "evalue": "400 models/gemini-2.5-flash is not found for CREATE TUNED MODEL at API version v1beta.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFailedPrecondition\u001B[39m                        Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[35]\u001B[39m\u001B[32m, line 26\u001B[39m\n\u001B[32m      5\u001B[39m base_model = \u001B[33m\"\u001B[39m\u001B[33mmodels/gemini-2.5-flash\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m      6\u001B[39m training_data = [\n\u001B[32m      7\u001B[39m     {\u001B[33m\"\u001B[39m\u001B[33mtext_input\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33m1\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33moutput\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33m2\u001B[39m\u001B[33m\"\u001B[39m},\n\u001B[32m      8\u001B[39m     {\u001B[33m\"\u001B[39m\u001B[33mtext_input\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33m2\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33moutput\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33m3\u001B[39m\u001B[33m\"\u001B[39m},\n\u001B[32m   (...)\u001B[39m\u001B[32m     23\u001B[39m     {\u001B[33m\"\u001B[39m\u001B[33mtext_input\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mnineteen\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33moutput\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mtwenty\u001B[39m\u001B[33m\"\u001B[39m}\n\u001B[32m     24\u001B[39m ]\n\u001B[32m---> \u001B[39m\u001B[32m26\u001B[39m operation = \u001B[43mgenai\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcreate_tuned_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     27\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# You can use a tuned model here too. Set `source_model=\"tunedModels/...\"`\u001B[39;49;00m\n\u001B[32m     28\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdisplay_name\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mincrement\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     29\u001B[39m \u001B[43m    \u001B[49m\u001B[43msource_model\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbase_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     30\u001B[39m \u001B[43m    \u001B[49m\u001B[43mepoch_count\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m10\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     31\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m4\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     32\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     33\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtraining_data\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtraining_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     34\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     36\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m status \u001B[38;5;129;01min\u001B[39;00m operation.wait_bar():\n\u001B[32m     37\u001B[39m     time.sleep(\u001B[32m10\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\deeplearning\\.venv\\Lib\\site-packages\\google\\generativeai\\models.py:364\u001B[39m, in \u001B[36mcreate_tuned_model\u001B[39m\u001B[34m(source_model, training_data, id, display_name, description, temperature, top_p, top_k, epoch_count, batch_size, learning_rate, input_key, output_key, client, request_options)\u001B[39m\n\u001B[32m    349\u001B[39m tuning_task = protos.TuningTask(\n\u001B[32m    350\u001B[39m     training_data=training_data,\n\u001B[32m    351\u001B[39m     hyperparameters=hyperparameters,\n\u001B[32m    352\u001B[39m )\n\u001B[32m    354\u001B[39m tuned_model = protos.TunedModel(\n\u001B[32m    355\u001B[39m     **source_model,\n\u001B[32m    356\u001B[39m     display_name=display_name,\n\u001B[32m   (...)\u001B[39m\u001B[32m    361\u001B[39m     tuning_task=tuning_task,\n\u001B[32m    362\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m364\u001B[39m operation = \u001B[43mclient\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcreate_tuned_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    365\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mdict\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtuned_model_id\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mid\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtuned_model\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtuned_model\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mrequest_options\u001B[49m\n\u001B[32m    366\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    368\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m operations.CreateTunedModelOperation.from_core_operation(operation)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\deeplearning\\.venv\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\model_service\\client.py:1294\u001B[39m, in \u001B[36mModelServiceClient.create_tuned_model\u001B[39m\u001B[34m(self, request, tuned_model, tuned_model_id, retry, timeout, metadata)\u001B[39m\n\u001B[32m   1291\u001B[39m \u001B[38;5;28mself\u001B[39m._validate_universe_domain()\n\u001B[32m   1293\u001B[39m \u001B[38;5;66;03m# Send the request.\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1294\u001B[39m response = \u001B[43mrpc\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1295\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1296\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretry\u001B[49m\u001B[43m=\u001B[49m\u001B[43mretry\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1297\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1298\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1299\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1301\u001B[39m \u001B[38;5;66;03m# Wrap the response in an operation future.\u001B[39;00m\n\u001B[32m   1302\u001B[39m response = operation.from_gapic(\n\u001B[32m   1303\u001B[39m     response,\n\u001B[32m   1304\u001B[39m     \u001B[38;5;28mself\u001B[39m._transport.operations_client,\n\u001B[32m   1305\u001B[39m     gag_tuned_model.TunedModel,\n\u001B[32m   1306\u001B[39m     metadata_type=model_service.CreateTunedModelMetadata,\n\u001B[32m   1307\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\deeplearning\\.venv\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001B[39m, in \u001B[36m_GapicCallable.__call__\u001B[39m\u001B[34m(self, timeout, retry, compression, *args, **kwargs)\u001B[39m\n\u001B[32m    128\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compression \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    129\u001B[39m     kwargs[\u001B[33m\"\u001B[39m\u001B[33mcompression\u001B[39m\u001B[33m\"\u001B[39m] = compression\n\u001B[32m--> \u001B[39m\u001B[32m131\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped_func\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\deeplearning\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001B[39m, in \u001B[36mRetry.__call__.<locals>.retry_wrapped_func\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    290\u001B[39m target = functools.partial(func, *args, **kwargs)\n\u001B[32m    291\u001B[39m sleep_generator = exponential_sleep_generator(\n\u001B[32m    292\u001B[39m     \u001B[38;5;28mself\u001B[39m._initial, \u001B[38;5;28mself\u001B[39m._maximum, multiplier=\u001B[38;5;28mself\u001B[39m._multiplier\n\u001B[32m    293\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m294\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mretry_target\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    295\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    296\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_predicate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    297\u001B[39m \u001B[43m    \u001B[49m\u001B[43msleep_generator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    298\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    299\u001B[39m \u001B[43m    \u001B[49m\u001B[43mon_error\u001B[49m\u001B[43m=\u001B[49m\u001B[43mon_error\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    300\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\deeplearning\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:156\u001B[39m, in \u001B[36mretry_target\u001B[39m\u001B[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001B[39m\n\u001B[32m    152\u001B[39m \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[32m    153\u001B[39m \u001B[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001B[39;00m\n\u001B[32m    154\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m    155\u001B[39m     \u001B[38;5;66;03m# defer to shared logic for handling errors\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m156\u001B[39m     next_sleep = \u001B[43m_retry_error_helper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    157\u001B[39m \u001B[43m        \u001B[49m\u001B[43mexc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    158\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdeadline\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    159\u001B[39m \u001B[43m        \u001B[49m\u001B[43msleep_iter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    160\u001B[39m \u001B[43m        \u001B[49m\u001B[43merror_list\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    161\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpredicate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    162\u001B[39m \u001B[43m        \u001B[49m\u001B[43mon_error\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    163\u001B[39m \u001B[43m        \u001B[49m\u001B[43mexception_factory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    164\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    165\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    166\u001B[39m     \u001B[38;5;66;03m# if exception not raised, sleep before next attempt\u001B[39;00m\n\u001B[32m    167\u001B[39m     time.sleep(next_sleep)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\deeplearning\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py:214\u001B[39m, in \u001B[36m_retry_error_helper\u001B[39m\u001B[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001B[39m\n\u001B[32m    208\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m predicate_fn(exc):\n\u001B[32m    209\u001B[39m     final_exc, source_exc = exc_factory_fn(\n\u001B[32m    210\u001B[39m         error_list,\n\u001B[32m    211\u001B[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001B[32m    212\u001B[39m         original_timeout,\n\u001B[32m    213\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m214\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m final_exc \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msource_exc\u001B[39;00m\n\u001B[32m    215\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m on_error_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    216\u001B[39m     on_error_fn(exc)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\deeplearning\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001B[39m, in \u001B[36mretry_target\u001B[39m\u001B[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001B[39m\n\u001B[32m    145\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m    146\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m147\u001B[39m         result = \u001B[43mtarget\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    148\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m inspect.isawaitable(result):\n\u001B[32m    149\u001B[39m             warnings.warn(_ASYNC_RETRY_WARNING)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\deeplearning\\.venv\\Lib\\site-packages\\google\\api_core\\timeout.py:130\u001B[39m, in \u001B[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    126\u001B[39m         remaining_timeout = \u001B[38;5;28mself\u001B[39m._timeout\n\u001B[32m    128\u001B[39m     kwargs[\u001B[33m\"\u001B[39m\u001B[33mtimeout\u001B[39m\u001B[33m\"\u001B[39m] = remaining_timeout\n\u001B[32m--> \u001B[39m\u001B[32m130\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\deeplearning\\.venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:77\u001B[39m, in \u001B[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m     75\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m callable_(*args, **kwargs)\n\u001B[32m     76\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m grpc.RpcError \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m---> \u001B[39m\u001B[32m77\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exceptions.from_grpc_error(exc) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mexc\u001B[39;00m\n",
      "\u001B[31mFailedPrecondition\u001B[39m: 400 models/gemini-2.5-flash is not found for CREATE TUNED MODEL at API version v1beta."
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "model = genai.GenerativeModel(model_name=\"tunedModels/increment-29dov0wxblr4\")\n",
    "result = model.generate_content(\"III\")\n",
    "print(result.text)  # \"IV\""
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T14:49:43.628234Z",
     "start_time": "2025-12-31T14:49:43.031606Z"
    }
   },
   "outputs": [
    {
     "ename": "NotFound",
     "evalue": "404 Tuned model tunedModels/increment-29dov0wxblr4 does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNotFound\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[30]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mgoogle\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mgenerativeai\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mgenai\u001B[39;00m\n\u001B[32m      3\u001B[39m model = genai.GenerativeModel(model_name=\u001B[33m\"\u001B[39m\u001B[33mtunedModels/increment-29dov0wxblr4\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m result = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate_content\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mIII\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[38;5;28mprint\u001B[39m(result.text)  \u001B[38;5;66;03m# \"IV\"\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\deeplearning\\.venv\\Lib\\site-packages\\google\\generativeai\\generative_models.py:331\u001B[39m, in \u001B[36mGenerativeModel.generate_content\u001B[39m\u001B[34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001B[39m\n\u001B[32m    329\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m generation_types.GenerateContentResponse.from_iterator(iterator)\n\u001B[32m    330\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m331\u001B[39m         response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_client\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate_content\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    332\u001B[39m \u001B[43m            \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    333\u001B[39m \u001B[43m            \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mrequest_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    334\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    335\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m generation_types.GenerateContentResponse.from_response(response)\n\u001B[32m    336\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m google.api_core.exceptions.InvalidArgument \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\deeplearning\\.venv\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:835\u001B[39m, in \u001B[36mGenerativeServiceClient.generate_content\u001B[39m\u001B[34m(self, request, model, contents, retry, timeout, metadata)\u001B[39m\n\u001B[32m    832\u001B[39m \u001B[38;5;28mself\u001B[39m._validate_universe_domain()\n\u001B[32m    834\u001B[39m \u001B[38;5;66;03m# Send the request.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m835\u001B[39m response = \u001B[43mrpc\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    836\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    837\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretry\u001B[49m\u001B[43m=\u001B[49m\u001B[43mretry\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    838\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    839\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    840\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    842\u001B[39m \u001B[38;5;66;03m# Done; return the response.\u001B[39;00m\n\u001B[32m    843\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\deeplearning\\.venv\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001B[39m, in \u001B[36m_GapicCallable.__call__\u001B[39m\u001B[34m(self, timeout, retry, compression, *args, **kwargs)\u001B[39m\n\u001B[32m    128\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compression \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    129\u001B[39m     kwargs[\u001B[33m\"\u001B[39m\u001B[33mcompression\u001B[39m\u001B[33m\"\u001B[39m] = compression\n\u001B[32m--> \u001B[39m\u001B[32m131\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped_func\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\deeplearning\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001B[39m, in \u001B[36mRetry.__call__.<locals>.retry_wrapped_func\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    290\u001B[39m target = functools.partial(func, *args, **kwargs)\n\u001B[32m    291\u001B[39m sleep_generator = exponential_sleep_generator(\n\u001B[32m    292\u001B[39m     \u001B[38;5;28mself\u001B[39m._initial, \u001B[38;5;28mself\u001B[39m._maximum, multiplier=\u001B[38;5;28mself\u001B[39m._multiplier\n\u001B[32m    293\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m294\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mretry_target\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    295\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    296\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_predicate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    297\u001B[39m \u001B[43m    \u001B[49m\u001B[43msleep_generator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    298\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    299\u001B[39m \u001B[43m    \u001B[49m\u001B[43mon_error\u001B[49m\u001B[43m=\u001B[49m\u001B[43mon_error\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    300\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\deeplearning\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:156\u001B[39m, in \u001B[36mretry_target\u001B[39m\u001B[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001B[39m\n\u001B[32m    152\u001B[39m \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[32m    153\u001B[39m \u001B[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001B[39;00m\n\u001B[32m    154\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m    155\u001B[39m     \u001B[38;5;66;03m# defer to shared logic for handling errors\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m156\u001B[39m     next_sleep = \u001B[43m_retry_error_helper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    157\u001B[39m \u001B[43m        \u001B[49m\u001B[43mexc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    158\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdeadline\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    159\u001B[39m \u001B[43m        \u001B[49m\u001B[43msleep_iter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    160\u001B[39m \u001B[43m        \u001B[49m\u001B[43merror_list\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    161\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpredicate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    162\u001B[39m \u001B[43m        \u001B[49m\u001B[43mon_error\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    163\u001B[39m \u001B[43m        \u001B[49m\u001B[43mexception_factory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    164\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    165\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    166\u001B[39m     \u001B[38;5;66;03m# if exception not raised, sleep before next attempt\u001B[39;00m\n\u001B[32m    167\u001B[39m     time.sleep(next_sleep)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\deeplearning\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py:214\u001B[39m, in \u001B[36m_retry_error_helper\u001B[39m\u001B[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001B[39m\n\u001B[32m    208\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m predicate_fn(exc):\n\u001B[32m    209\u001B[39m     final_exc, source_exc = exc_factory_fn(\n\u001B[32m    210\u001B[39m         error_list,\n\u001B[32m    211\u001B[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001B[32m    212\u001B[39m         original_timeout,\n\u001B[32m    213\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m214\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m final_exc \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msource_exc\u001B[39;00m\n\u001B[32m    215\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m on_error_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    216\u001B[39m     on_error_fn(exc)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\deeplearning\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001B[39m, in \u001B[36mretry_target\u001B[39m\u001B[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001B[39m\n\u001B[32m    145\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m    146\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m147\u001B[39m         result = \u001B[43mtarget\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    148\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m inspect.isawaitable(result):\n\u001B[32m    149\u001B[39m             warnings.warn(_ASYNC_RETRY_WARNING)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\deeplearning\\.venv\\Lib\\site-packages\\google\\api_core\\timeout.py:130\u001B[39m, in \u001B[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    126\u001B[39m         remaining_timeout = \u001B[38;5;28mself\u001B[39m._timeout\n\u001B[32m    128\u001B[39m     kwargs[\u001B[33m\"\u001B[39m\u001B[33mtimeout\u001B[39m\u001B[33m\"\u001B[39m] = remaining_timeout\n\u001B[32m--> \u001B[39m\u001B[32m130\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\deeplearning\\.venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:77\u001B[39m, in \u001B[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m     75\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m callable_(*args, **kwargs)\n\u001B[32m     76\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m grpc.RpcError \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m---> \u001B[39m\u001B[32m77\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exceptions.from_grpc_error(exc) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mexc\u001B[39;00m\n",
      "\u001B[31mNotFound\u001B[39m: 404 Tuned model tunedModels/increment-29dov0wxblr4 does not exist."
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mettre à jour la description"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "genai.update_tuned_model('tunedModels/my-increment-model', {\"description\":\"This is my model.\"})"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Supprimer le modèle"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "genai.delete_tuned_model(\"tunedModels/my-increment-model\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "genai.delete_tuned_model(\"tunedModels/increment-29dov0wxblr4\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for model_info in genai.list_tuned_models():\n",
    "    print(model_info.name)"
   ],
   "metadata": {},
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bousmah_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
